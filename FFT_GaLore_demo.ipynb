{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/SurgicalVLM-Agent/blob/main/FFT_GaLore_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "2lNWIMo7MW_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Surgical LLM Agent Demo\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directories in Google Drive\n",
        "import os\n",
        "output_base_dir = '/content/drive/MyDrive/surgical_llm_demo'\n",
        "model_dir = f'{output_base_dir}/models/r128'\n",
        "data_dir = f'{output_base_dir}/datasets'\n",
        "results_dir = f'{output_base_dir}/results'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for directory in [output_base_dir, model_dir, data_dir, results_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Created output directories in Google Drive at: {output_base_dir}\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers datasets evaluate gdown nltk torch\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q accelerate\n",
        "\n",
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download model weights using gdown (NOTE: Replace with your actual file ID)\n",
        "print(\"Downloading model weights...\")\n",
        "MODEL_FILE_ID = \"YOUR_MODEL_FILE_ID_HERE\"  # Replace with the actual file ID from Google Drive\n",
        "\n",
        "# Check if model is already downloaded\n",
        "if not os.path.exists(f\"{model_dir}/config.json\"):\n",
        "    !gdown --folder {MODEL_FILE_ID} -O {model_dir}\n",
        "    print(f\"Model weights downloaded to {model_dir}\")\n",
        "else:\n",
        "    print(f\"Model weights already exist at {model_dir}\")\n",
        "\n",
        "# Download datasets\n",
        "print(\"Downloading datasets...\")\n",
        "DATASET_FILE_IDS = {\n",
        "    \"Surgical-VQA_V.csv\": \"1rjv3PzKHqz5BjJn8anR2jSwNTPS2k0Ak\",\n",
        "    \"Overlaying_V.csv\": \"1gFpt8kjoc0kzTBXXiRlYgwDC-HzlSGAr\",\n",
        "    \"Segment-MRI_V.csv\": \"1rSJfPEqg24fhk4MybqpRw652orLfgojc\",\n",
        "    \"Segment-Video_V.csv\": \"1lo0xEKcJgMPy0T0AXfRIjNbrSdJyrXkR\",\n",
        "    \"Detect-Instrument_V.csv\": \"1A4c5ieW6P_oqMnWRybl6NmtmTshsPX1n\",\n",
        "    \"2model_V.csv\": \"1dl_81gH1o06ZYLn1FuL8J4INxq3cLqnu\",\n",
        "    \"3model_V.csv\": \"1WwLigg0kjRHyxkOaSYc8V2M8GFK9qlc9\"\n",
        "}\n",
        "\n",
        "# https://drive.google.com/file/d/1rjv3PzKHqz5BjJn8anR2jSwNTPS2k0Ak/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1gFpt8kjoc0kzTBXXiRlYgwDC-HzlSGAr/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1rSJfPEqg24fhk4MybqpRw652orLfgojc/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1lo0xEKcJgMPy0T0AXfRIjNbrSdJyrXkR/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1A4c5ieW6P_oqMnWRybl6NmtmTshsPX1n/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1dl_81gH1o06ZYLn1FuL8J4INxq3cLqnu/view?usp=drive_link\n",
        "# https://drive.google.com/file/d/1WwLigg0kjRHyxkOaSYc8V2M8GFK9qlc9/view?usp=drive_link\n",
        "\n",
        "# Download each dataset\n",
        "for filename, file_id in DATASET_FILE_IDS.items():\n",
        "    if not os.path.exists(f\"{data_dir}/{filename}\"):\n",
        "        !gdown {file_id} -O {data_dir}/{filename}\n",
        "        print(f\"Downloaded {filename}\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists\")"
      ],
      "metadata": {
        "id": "sd40JekWMYkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca23381e-8c29-455a-8889-576bce8f50d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Created output directories in Google Drive at: /content/drive/MyDrive/surgical_llm_demo\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m784.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading model weights...\n",
            "Model weights already exist at /content/drive/MyDrive/surgical_llm_demo/models/r128\n",
            "Downloading datasets...\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rjv3PzKHqz5BjJn8anR2jSwNTPS2k0Ak\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/Surgical-VQA_V.csv\n",
            "100% 316k/316k [00:00<00:00, 5.33MB/s]\n",
            "Downloaded Surgical-VQA_V.csv\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gFpt8kjoc0kzTBXXiRlYgwDC-HzlSGAr\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/Overlaying_V.csv\n",
            "100% 129k/129k [00:00<00:00, 5.30MB/s]\n",
            "Downloaded Overlaying_V.csv\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rSJfPEqg24fhk4MybqpRw652orLfgojc\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/Segment-MRI_V.csv\n",
            "100% 113k/113k [00:00<00:00, 4.39MB/s]\n",
            "Downloaded Segment-MRI_V.csv\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lo0xEKcJgMPy0T0AXfRIjNbrSdJyrXkR\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/Segment-Video_V.csv\n",
            "100% 144k/144k [00:00<00:00, 4.43MB/s]\n",
            "Downloaded Segment-Video_V.csv\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1A4c5ieW6P_oqMnWRybl6NmtmTshsPX1n\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/Detect-Instrument_V.csv\n",
            "100% 120k/120k [00:00<00:00, 5.35MB/s]\n",
            "Downloaded Detect-Instrument_V.csv\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dl_81gH1o06ZYLn1FuL8J4INxq3cLqnu\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/2model_V.csv\n",
            "100% 265k/265k [00:00<00:00, 4.95MB/s]\n",
            "Downloaded 2model_V.csv\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WwLigg0kjRHyxkOaSYc8V2M8GFK9qlc9\n",
            "To: /content/drive/MyDrive/surgical_llm_demo/datasets/3model_V.csv\n",
            "100% 395k/395k [00:00<00:00, 5.67MB/s]\n",
            "Downloaded 3model_V.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "htSwrhJ_OhAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import evaluate\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from tqdm.notebook import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ---------------------------- Utility Functions ----------------------------\n",
        "\n",
        "class TextQuestionLabelDataset(Dataset):\n",
        "    def __init__(self, input_file):\n",
        "        self.data = pd.read_csv(input_file)\n",
        "        self.questions = self.data['Input'].tolist()\n",
        "        self.labels = self.data['Label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.labels[idx]\n",
        "\n",
        "def generate1_SM(que):\n",
        "    sm = (\n",
        "        \"Select a model to answer the following question:\\n\"\n",
        "        f\"'{que}'\\n\"\n",
        "        \"Select a model only from the following model list: Segment-Video, Segment-MRI, \"\n",
        "        \"Detect-Instrument, Overlaying, Visual-Question-Answering to answer the question.\\n\"\n",
        "        \"Generate a shortest prompt for the chosen model. \"\n",
        "        \"Your response should be like Model: ? Prompt: ? and no other words.\"\n",
        "    )\n",
        "    return sm\n",
        "\n",
        "def generate2_SM(que):\n",
        "    sm = (\n",
        "        \"Select models to answer the following question:\\n\"\n",
        "        f\"'{que}'\\n\"\n",
        "        \"Select appropriate models based on the question and select them only from the following model list: [Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Visual-Question-Answering] \\n\"\n",
        "        \"Generate shortest prompts for selected models.\\n\"\n",
        "        \"The format of your response should be like: 'Step1: Model: ? Prompt: ?...' Use as many steps as necessary, and no other words.\"\n",
        "    )\n",
        "    return sm\n",
        "\n",
        "def format_data(sample):\n",
        "    if \"Step1\" in sample[1]:\n",
        "        system_message = generate2_SM(sample[0])\n",
        "    else:\n",
        "        system_message = generate1_SM(sample[0])\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"assistant\", \"content\": sample[1]}\n",
        "    ]\n",
        "\n",
        "def custom_collate_fn(sample):\n",
        "    # Keep DataLoader sample format unchanged\n",
        "    return sample\n",
        "\n",
        "def generate_answer(question, model, tokenizer):\n",
        "    model.eval()\n",
        "    input_text = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "def extract_prompt(entry):\n",
        "    # Extract all lines starting with \"Prompt:\" and join with \"|\"\n",
        "    prompts = [line[len(\"Prompt: \"):].strip() for line in entry.split('\\n') if line.startswith(\"Prompt:\")]\n",
        "    return \"|\".join(prompts) if prompts else \"\"\n",
        "\n",
        "def group_by_sentence_position(all_prompts, num_sentences):\n",
        "    grouped_sentences = [[] for _ in range(num_sentences)]\n",
        "    for prompts in all_prompts:\n",
        "        sentences = prompts.split(\"|\")\n",
        "        for i in range(num_sentences):\n",
        "            if i < len(sentences):\n",
        "                grouped_sentences[i].append(sentences[i])\n",
        "            else:\n",
        "                grouped_sentences[i].append(\"\")\n",
        "    return grouped_sentences\n",
        "\n",
        "def compute_metrics(grouped_pred_prompts, grouped_ans_prompts):\n",
        "    \"\"\"\n",
        "    Compute ROUGE, BLEU and METEOR metrics using Hugging Face evaluate library\n",
        "    \"\"\"\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "    bleu_scores = []\n",
        "    meteor_scores = []\n",
        "    rouge_results = []\n",
        "\n",
        "    for i in range(len(grouped_pred_prompts)):\n",
        "        pred_group = grouped_pred_prompts[i]\n",
        "        ans_group = grouped_ans_prompts[i]\n",
        "\n",
        "        # ROUGE: Pass predictions and references lists directly\n",
        "        rouge_result = rouge.compute(predictions=pred_group, references=ans_group)\n",
        "        rouge_results.append(rouge_result)\n",
        "\n",
        "        # BLEU: References need to be nested lists\n",
        "        bleu_result = bleu.compute(predictions=pred_group, references=[[ref] for ref in ans_group])\n",
        "        bleu_scores.append({\n",
        "            \"bleu1\": bleu_result[\"precisions\"][0],\n",
        "            \"bleu2\": bleu_result[\"precisions\"][1],\n",
        "            \"bleu3\": bleu_result[\"precisions\"][2],\n",
        "            \"bleu4\": bleu_result[\"precisions\"][3]\n",
        "        })\n",
        "\n",
        "        # METEOR: Pass predictions and references lists directly\n",
        "        meteor_result = meteor.compute(predictions=pred_group, references=ans_group)\n",
        "        meteor_scores.append(meteor_result[\"meteor\"])\n",
        "\n",
        "    return rouge_results, bleu_scores, meteor_scores\n",
        "\n",
        "def extract_model(text, model_names):\n",
        "    # Use regex to match model names from the list\n",
        "    matches = re.findall(r'\\b(?:' + '|'.join(map(re.escape, model_names)) + r')\\b', text)\n",
        "    return \"|\".join(matches) if matches else \"\"\n",
        "\n",
        "def match_rate_per_Cat(pred_models_format, true_models_format):\n",
        "    # Initialize counters\n",
        "    first_model_match_count = 0\n",
        "    second_model_match_count = 0\n",
        "    third_model_match_count = 0\n",
        "    total_count = len(true_models_format)\n",
        "\n",
        "    # Iterate through both lists\n",
        "    for pred, true in zip(pred_models_format, true_models_format):\n",
        "        # Split model names\n",
        "        pred_models = pred.split(\"|\")\n",
        "        true_models = true.split(\"|\")\n",
        "\n",
        "        # Pad predictions if needed\n",
        "        while len(pred_models) < len(true_models):\n",
        "            pred_models.append(\" \")\n",
        "\n",
        "        # Check first model match\n",
        "        if len(true_models) > 0 and pred_models[0] == true_models[0]:\n",
        "            first_model_match_count += 1\n",
        "\n",
        "        # Check second model match\n",
        "        if len(true_models) > 1 and pred_models[1] == true_models[1]:\n",
        "            second_model_match_count += 1\n",
        "\n",
        "        # Check third model match\n",
        "        if len(true_models) > 2 and pred_models[2] == true_models[2]:\n",
        "            third_model_match_count += 1\n",
        "\n",
        "    # Calculate match percentages\n",
        "    first_model_match_rate = (first_model_match_count / total_count * 100) if total_count > 0 else 0\n",
        "    second_model_match_rate = (second_model_match_count / total_count * 100) if total_count > 0 else 0\n",
        "    third_model_match_rate = (third_model_match_count / total_count * 100) if total_count > 0 else 0\n",
        "\n",
        "    return first_model_match_rate, second_model_match_rate, third_model_match_rate\n",
        "\n",
        "def f1_score_set(pred_list, true_list):\n",
        "    pred_set = set(pred_list)\n",
        "    true_set = set(true_list)\n",
        "    tp = len(pred_set & true_set)\n",
        "    precision = tp / len(pred_set) if pred_set else 0\n",
        "    recall = tp / len(true_set) if true_set else 0\n",
        "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "def evaluate_f1_by_selection_count(pred_models_format, true_models_format):\n",
        "    \"\"\"\n",
        "    Calculate F1 scores for 1, 2, and 3 model selections\n",
        "    \"\"\"\n",
        "    one_model_scores = []\n",
        "    two_model_scores = []\n",
        "    three_model_scores = []\n",
        "\n",
        "    for pred, true in zip(pred_models_format, true_models_format):\n",
        "        # Split strings and remove extra spaces\n",
        "        pred_models = [m.strip() for m in pred.split(\"|\")]\n",
        "        true_models = [m.strip() for m in true.split(\"|\")]\n",
        "\n",
        "        # Pad predictions if needed\n",
        "        while len(pred_models) < len(true_models):\n",
        "            pred_models.append(\"\")\n",
        "\n",
        "        # Calculate F1 based on true model count\n",
        "        if len(true_models) == 1:\n",
        "            score = f1_score_set(pred_models[:1], true_models[:1])\n",
        "            one_model_scores.append(score)\n",
        "        elif len(true_models) == 2:\n",
        "            score = f1_score_set(pred_models[:2], true_models[:2])\n",
        "            two_model_scores.append(score)\n",
        "        elif len(true_models) == 3:\n",
        "            score = f1_score_set(pred_models[:3], true_models[:3])\n",
        "            three_model_scores.append(score)\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_one_model_f1 = sum(one_model_scores) / len(one_model_scores) if one_model_scores else 0\n",
        "    avg_two_model_f1 = sum(two_model_scores) / len(two_model_scores) if two_model_scores else 0\n",
        "    avg_three_model_f1 = sum(three_model_scores) / len(three_model_scores) if three_model_scores else 0\n",
        "\n",
        "    return avg_one_model_f1, avg_two_model_f1, avg_three_model_f1\n",
        "\n",
        "# ------------------------- Metric Extraction and Aggregation Functions -------------------------\n",
        "\n",
        "def extract_values(file_path):\n",
        "    \"\"\"\n",
        "    Read file contents and extract metric values:\n",
        "    - rouge1, rougeL: match after \"rouge1':\" and \"rougeL':\"\n",
        "    - BLEU: match after \"BLEU Score: [\"\n",
        "    - METEOR: match after \"METEOR Score: [\"\n",
        "    - F1: based on file name\n",
        "    - Matching Accuracy: match after \"Matching Accuracy of the 1st/2nd/3rd model:\"\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    rouge1_matches = re.findall(r\"rouge1'\\s*:\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "    rougeL_matches = re.findall(r\"rougeL'\\s*:\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "    bleu1_matches = re.findall(r\"bleu1'\\s*:\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "    bleu2_matches = re.findall(r\"bleu2'\\s*:\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "    bleu3_matches = re.findall(r\"bleu3'\\s*:\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "    bleu4_matches = re.findall(r\"bleu4'\\s*:\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "    meteor_matches = re.findall(r\"METEOR Score:\\s*\\[\\s*([0-9]*\\.?[0-9]+)\", content)\n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    first_char = file_name.lstrip()[0] if file_name.lstrip() else ''\n",
        "    if first_char == '2':\n",
        "        f1_pattern = r\"F1 score of two models:\\s*([0-9]*\\.?[0-9]+)\"\n",
        "    elif first_char == '3':\n",
        "        f1_pattern = r\"F1 score of three models:\\s*([0-9]*\\.?[0-9]+)\"\n",
        "    else:\n",
        "        f1_pattern = r\"F1 score of current model:\\s*([0-9]*\\.?[0-9]+)\"\n",
        "    f1_matches = re.findall(f1_pattern, content)\n",
        "\n",
        "    matching_accuracy_matches = re.findall(\n",
        "        r\"Matching Accuracy of(?:\\s+the)?\\s+\\d+(?:st|nd|rd|th)\\s+model:\\s*([0-9]*\\.?[0-9]+)%\",\n",
        "        content\n",
        "    )\n",
        "\n",
        "    def to_float(lst):\n",
        "        return [float(x) for x in lst]\n",
        "\n",
        "    return {\n",
        "        'rouge1': to_float(rouge1_matches),\n",
        "        'rougeL': to_float(rougeL_matches),\n",
        "        'bleu1': to_float(bleu1_matches),\n",
        "        'bleu2': to_float(bleu2_matches),\n",
        "        'bleu3': to_float(bleu3_matches),\n",
        "        'bleu4': to_float(bleu4_matches),\n",
        "        'METEOR': to_float(meteor_matches),\n",
        "        'F1': to_float(f1_matches),\n",
        "        'Matching_Accuracy': to_float(matching_accuracy_matches)\n",
        "    }\n",
        "\n",
        "def compute_average(values):\n",
        "    \"\"\"Calculate average of a list; return 0 if empty\"\"\"\n",
        "    return sum(values) / len(values) if values else 0\n",
        "\n",
        "def process_results_folder(results_folder):\n",
        "    \"\"\"\n",
        "    Process all evaluation files in the results folder,\n",
        "    extract metrics, and compute averages.\n",
        "    \"\"\"\n",
        "    all_values = {\n",
        "        'rouge1': [],\n",
        "        'rougeL': [],\n",
        "        'bleu1': [],\n",
        "        'bleu2': [],\n",
        "        'bleu3': [],\n",
        "        'bleu4': [],\n",
        "        'METEOR': [],\n",
        "        'F1': [],\n",
        "        'Matching_Accuracy': []\n",
        "    }\n",
        "\n",
        "    evaluation_files = [f for f in os.listdir(results_folder) if f.endswith(\"_evaluation.txt\")]\n",
        "\n",
        "    if not evaluation_files:\n",
        "        print(f\"No evaluation files found in {results_folder}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Processing {len(evaluation_files)} evaluation files from {results_folder}\")\n",
        "\n",
        "    for file in evaluation_files:\n",
        "        file_path = os.path.join(results_folder, file)\n",
        "        vals = extract_values(file_path)\n",
        "        for key in all_values:\n",
        "            all_values[key].extend(vals[key])\n",
        "\n",
        "    # Show extracted F1 values for debugging\n",
        "    print(f\"F1 values: {all_values['F1']}\")\n",
        "\n",
        "    avg_metrics = {key: compute_average(all_values[key]) for key in all_values}\n",
        "    return avg_metrics\n",
        "\n",
        "def save_and_display_metrics(avg_metrics, output_dir, model_name=\"surgical_llm\"):\n",
        "    \"\"\"\n",
        "    Save metrics to TXT and CSV files, and display in the notebook\n",
        "    \"\"\"\n",
        "    if avg_metrics is None:\n",
        "        print(\"No metrics to save or display.\")\n",
        "        return\n",
        "\n",
        "    # Save TXT file\n",
        "    txt_file = os.path.join(output_dir, f\"{model_name}_metrics.txt\")\n",
        "    with open(txt_file, 'w', encoding='utf-8') as out:\n",
        "        out.write(\"Average rouge1: {:.4f}\\n\".format(avg_metrics['rouge1']))\n",
        "        out.write(\"Average rougeL: {:.4f}\\n\".format(avg_metrics['rougeL']))\n",
        "        out.write(\"Average bleu1 Score: {:.4f}\\n\".format(avg_metrics['bleu1']))\n",
        "        out.write(\"Average bleu2 Score: {:.4f}\\n\".format(avg_metrics['bleu2']))\n",
        "        out.write(\"Average bleu3 Score: {:.4f}\\n\".format(avg_metrics['bleu3']))\n",
        "        out.write(\"Average bleu4 Score: {:.4f}\\n\".format(avg_metrics['bleu4']))\n",
        "        out.write(\"Average METEOR Score: {:.4f}\\n\".format(avg_metrics['METEOR']))\n",
        "        out.write(\"Average F1 Score: {:.4f}\\n\".format(avg_metrics['F1']))\n",
        "        out.write(\"Average Matching Accuracy: {:.4f}\\n\".format(avg_metrics['Matching_Accuracy']))\n",
        "\n",
        "    print(f\"Metrics saved to {txt_file}\")\n",
        "\n",
        "    # Save CSV file\n",
        "    csv_file = os.path.join(output_dir, f\"{model_name}_metrics.csv\")\n",
        "    with open(csv_file, 'w', encoding='utf-8') as csv_out:\n",
        "        # CSV headers\n",
        "        headers = [\n",
        "            \"Average bleu1 Score\", \"Average bleu2 Score\", \"Average bleu3 Score\", \"Average bleu4 Score\",\n",
        "            \"Average rouge1\", \"Average rougeL\", \"Average METEOR Score\",\n",
        "            \"Average F1 Score\", \"Average Matching Accuracy\"\n",
        "        ]\n",
        "\n",
        "        # Convert to percentages (multiply by 100)\n",
        "        values = [\n",
        "            avg_metrics['bleu1'] * 100,\n",
        "            avg_metrics['bleu2'] * 100,\n",
        "            avg_metrics['bleu3'] * 100,\n",
        "            avg_metrics['bleu4'] * 100,\n",
        "            avg_metrics['rouge1'] * 100,\n",
        "            avg_metrics['rougeL'] * 100,\n",
        "            avg_metrics['METEOR'] * 100,\n",
        "            avg_metrics['F1'] * 100,\n",
        "            avg_metrics['Matching_Accuracy']  # Already a percentage\n",
        "        ]\n",
        "\n",
        "        # Write to CSV\n",
        "        csv_out.write(\",\".join(headers) + \"\\n\")\n",
        "        csv_out.write(\",\".join(\"{:.2f}\".format(val) for val in values) + \"\\n\")\n",
        "\n",
        "    print(f\"CSV metrics saved to {csv_file}\")\n",
        "\n",
        "    # Display metrics in a nice table in the notebook\n",
        "    print(\"\\n===== AVERAGE METRICS =====\")\n",
        "    metric_df = pd.DataFrame({\n",
        "        'Metric': [\n",
        "            'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4',\n",
        "            'ROUGE-1', 'ROUGE-L', 'METEOR',\n",
        "            'F1 Score', 'Matching Accuracy'\n",
        "        ],\n",
        "        'Value (%)': [\n",
        "            avg_metrics['bleu1'] * 100,\n",
        "            avg_metrics['bleu2'] * 100,\n",
        "            avg_metrics['bleu3'] * 100,\n",
        "            avg_metrics['bleu4'] * 100,\n",
        "            avg_metrics['rouge1'] * 100,\n",
        "            avg_metrics['rougeL'] * 100,\n",
        "            avg_metrics['METEOR'] * 100,\n",
        "            avg_metrics['F1'] * 100,\n",
        "            avg_metrics['Matching_Accuracy']\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    display(metric_df)\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bar_colors = ['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4',\n",
        "                 '#ff7f0e', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "    bars = plt.bar(metric_df['Metric'], metric_df['Value (%)'], color=bar_colors)\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.ylabel('Value (%)')\n",
        "    plt.title('Average Evaluation Metrics')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylim(0, 100)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                 f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, f\"{model_name}_metrics_plot.png\"), dpi=300)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JFq_GNIG7k7F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "5wvkS6Hm7yg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------- Main Execution --------------------------\n",
        "\n",
        "# Load the model using BitsAndBytes for efficient loading\n",
        "print(\"Loading model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Get list of input files\n",
        "    input_files = [f\"{data_dir}/{filename}\" for filename in os.listdir(data_dir) if filename.endswith('.csv')]\n",
        "    print(f\"Found {len(input_files)} input files: {[os.path.basename(f) for f in input_files]}\")\n",
        "\n",
        "    # Process each input file\n",
        "    for input_file in input_files:\n",
        "        print(f\"\\nProcessing file: {os.path.basename(input_file)}\")\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        dataset = TextQuestionLabelDataset(input_file)\n",
        "        test_dataset = [format_data(sample) for sample in dataset]\n",
        "        batch_size = 6  # Adjust based on your GPU memory\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "        all_pred = []\n",
        "        all_ans = []\n",
        "\n",
        "        # Run inference\n",
        "        model.eval()\n",
        "        print(\"Running inference...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Inference\"):\n",
        "                temp_pred = []\n",
        "                temp_ans = []\n",
        "                for sample in batch:\n",
        "                    output = generate_answer(sample[0]['content'], model, tokenizer)\n",
        "                    ans = sample[1]['content']\n",
        "                    temp_pred.append(output)\n",
        "                    temp_ans.append(ans)\n",
        "                all_pred.extend(temp_pred)\n",
        "                all_ans.extend(temp_ans)\n",
        "\n",
        "        # Save predictions to file\n",
        "        base_filename = os.path.basename(input_file).replace(\".csv\", \"\")\n",
        "        pred_output_file = os.path.join(results_dir, f\"{base_filename}_pred.txt\")\n",
        "        with open(pred_output_file, \"w\") as f:\n",
        "            for pred_text, ans_text in zip(all_pred, all_ans):\n",
        "                f.write(f\"pred: {pred_text}\\n\")\n",
        "                f.write(f\"ans: {ans_text}\\n\\n\")\n",
        "        print(f\"Saved predictions to {pred_output_file}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(\"Calculating metrics...\")\n",
        "\n",
        "        # Process prompts for evaluation\n",
        "        all_pred_prompts = all_pred\n",
        "        all_ans_prompts = all_ans\n",
        "\n",
        "        # Determine the number of sentences in the first answer\n",
        "        # For robust handling, we'll use a default of 1 if splitting fails\n",
        "        try:\n",
        "            num_sentences = len(all_ans_prompts[0].split(\"|\"))\n",
        "        except:\n",
        "            num_sentences = 1\n",
        "            print(\"Warning: Could not determine number of sentences, using default of 1\")\n",
        "\n",
        "        grouped_pred_prompts = group_by_sentence_position(all_pred_prompts, num_sentences)\n",
        "        grouped_ans_prompts = group_by_sentence_position(all_ans_prompts, num_sentences)\n",
        "\n",
        "        rouge_results, bleu_scores, meteor_scores = compute_metrics(grouped_pred_prompts, grouped_ans_prompts)\n",
        "\n",
        "        # Evaluate model selection accuracy\n",
        "        model_names = [\"Segment-MRI\", \"Segment-Video\", \"Detect-Instrument\", \"Overlaying\", \"Visual-Question-Answering\"]\n",
        "        pred_models = [extract_model(pred, model_names=model_names).strip() for pred in all_pred]\n",
        "        true_models = [extract_model(ans, model_names=model_names).strip() for ans in all_ans]\n",
        "\n",
        "        # Get match rates\n",
        "        first_rate, second_rate, third_rate = match_rate_per_Cat(pred_models, true_models)\n",
        "\n",
        "        # Determine model number from first true model\n",
        "        true_models_list = true_models[0].split(\"|\") if true_models else []\n",
        "        model_num = len(true_models_list)\n",
        "\n",
        "        # Calculate F1 scores\n",
        "        avg_one_f1, avg_two_f1, avg_three_f1 = evaluate_f1_by_selection_count(pred_models, true_models)\n",
        "\n",
        "        # Save evaluation results\n",
        "        eval_output_file = os.path.join(results_dir, f\"{base_filename}_evaluation.txt\")\n",
        "        with open(eval_output_file, \"w\") as f:\n",
        "            f.write(f\"Rouge Scores: {rouge_results}\\n\")\n",
        "            f.write(f\"BLEU Score: {bleu_scores}\\n\")\n",
        "            f.write(f\"METEOR Score: {meteor_scores}\\n\")\n",
        "\n",
        "            if model_num > 0:\n",
        "                f.write(f\"Matching Accuracy of the 1st model: {first_rate:.2f}%\\n\")\n",
        "                f.write(f\"F1 score of current model: {avg_one_f1:.4f}\\n\")\n",
        "            if model_num > 1:\n",
        "                f.write(f\"Matching Accuracy of the 2nd model: {second_rate:.2f}%\\n\")\n",
        "                f.write(f\"F1 score of two models: {avg_two_f1:.4f}\\n\")\n",
        "            if model_num > 2:\n",
        "                f.write(f\"Matching Accuracy of the 3rd model: {third_rate:.2f}%\\n\")\n",
        "                f.write(f\"F1 score of three models: {avg_three_f1:.4f}\\n\")\n",
        "\n",
        "        print(f\"Saved evaluation results to {eval_output_file}\")\n",
        "\n",
        "    print(\"\\nAll files processed successfully!\")\n",
        "    print(f\"Results saved to {results_dir}\")\n",
        "\n",
        "    # Process results and calculate average metrics\n",
        "    print(\"\\nCalculating average metrics across all datasets...\")\n",
        "    avg_metrics = process_results_folder(results_dir)\n",
        "\n",
        "    # Save and display average metrics\n",
        "    save_and_display_metrics(avg_metrics, metrics_dir)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "X-sAaIYQ70sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9923e600-e663-4dcf-8801-82dd9e1427d1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-27-3a63f617f6ac>\", line 14, in <cell line: 0>\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4228, in from_pretrained\n",
            "    hf_quantizer.validate_environment(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 84, in validate_environment\n",
            "    validate_bnb_backend_availability(raise_exception=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py\", line 561, in validate_bnb_backend_availability\n",
            "    return _validate_bnb_cuda_backend_availability(raise_exception)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py\", line 539, in _validate_bnb_cuda_backend_availability\n",
            "    raise RuntimeError(log_msg)\n",
            "RuntimeError: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cir1Sl-8t-F0"
      }
    }
  ]
}